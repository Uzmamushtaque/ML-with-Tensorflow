{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPwithTensorflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-3ovqlvLFYr",
        "outputId": "6c1df278-cd01-4c63-a292-0f1ddcc4b4dd"
      },
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-b54e83fe-0a3b-0124-679a-376eec338e7f)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5vNcpQALs-b"
      },
      "source": [
        "Binary classification using twitter data from Kaggle.https://www.kaggle.com/c/nlp-getting-started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx6aVLDTLUIC",
        "outputId": "a599662e-f5ed-4166-8a96-8f53f21a5909"
      },
      "source": [
        "# Download helper functions script\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-31 14:52:28--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-31 14:52:28 (102 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-blpiquLYUy"
      },
      "source": [
        "# Import series of helper functions for the notebook\n",
        "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIweTaUpLdu1",
        "outputId": "fbac0c71-a015-46b1-f716-0c3762594c30"
      },
      "source": [
        "# Download data (same as from Kaggle)\n",
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
        "\n",
        "# Unzip data\n",
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-31 14:52:31--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.135.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.135.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-07-31 14:52:31 (148 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iesWxWzMMQih"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DxOOhrjMmmw"
      },
      "source": [
        "Visualizing Text Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6RIeW0mvMsS9",
        "outputId": "21d07be9-c0d8-4969-bfa0-3e2aaa1e3fc4"
      },
      "source": [
        "# Turn .csv files into pandas DataFrame's\n",
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "train_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4OB0KIhINQcT",
        "outputId": "2add7afa-37db-4cd4-db98-33657a0ed9b5"
      },
      "source": [
        "# Shuffle training dataframe\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
        "train_df_shuffled.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... target\n",
              "2644  3796  ...      1\n",
              "2227  3185  ...      0\n",
              "5448  7769  ...      1\n",
              "132    191  ...      0\n",
              "6845  9810  ...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mWlEUQueNiWy",
        "outputId": "04c80473-cacf-4188-c040-f8409ede675f"
      },
      "source": [
        "# The test data doesn't have a target (that's what we'd try to predict)\n",
        "test_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRbkdzuJNpQ2",
        "outputId": "208d9ff3-8613-47bf-f760-c22383b4c145"
      },
      "source": [
        "# How many examples of each class?\n",
        "train_df.target.value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mgbVpVCNx6a",
        "outputId": "8fa62121-592f-4885-ab12-79a2f4106414"
      },
      "source": [
        "# How many samples total?\n",
        "print(f\"Total training samples: {len(train_df)}\")\n",
        "print(f\"Total test samples: {len(test_df)}\")\n",
        "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training samples: 7613\n",
            "Total test samples: 3263\n",
            "Total samples: 10876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yIwTqseRV78",
        "outputId": "66d1246c-e86b-4c3d-8876-74d6a5cea9c3"
      },
      "source": [
        "# Let's visualize some random training examples\n",
        "import random\n",
        "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
        "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            "@abcnews UK scandal of 2009 caused major upheaval to Parliamentary expenses with subsequent sackings and prison. What are we waiting for?\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "Text:\n",
            "being stuck on a sleeper train for 24 hours after de-railing due to a landslide was most definitely the pit of the trip\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "Text:\n",
            "http://t.co/iXiYBAp8Qa The Latest: More homes razed by Northern California wildfire - Lynchburg News and Advance http://t.co/zEpzQYDby4\n",
            "\n",
            "---\n",
            "\n",
            "Target: 1 (real disaster)\n",
            "Text:\n",
            "NWS has Continued a Severe Thunderstorm Warning for Oklahoma-OK until 10:30 PM\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            "seriously look like a get electrocuted after I blow dry my hair it's really attractive ??\n",
            "\n",
            "---\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKRvZ05-RkuB"
      },
      "source": [
        "**Split data into training and validation sets:**\n",
        "\n",
        "Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqXn5jk0Rto7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use train_test_split to split training data into training and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
        "                                                                            random_state=42) # random state for reproducibility"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4In71A4YqkWO",
        "outputId": "041c6409-0fa3-41ef-9d0d-8aa846934c83"
      },
      "source": [
        "# Check the lengths\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzDIpiSLqswz",
        "outputId": "e4a2c897-9e0a-4121-8bd5-da7740f1dcca"
      },
      "source": [
        "# View the first 10 training sentences and their labels\n",
        "train_sentences[:10], train_labels[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD5h2Q0zxaKX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Use the default TextVectorization\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                    split=\"whitespace\", # how to split tokens\n",
        "                                    ngrams=None, # create groups of n-words?\n",
        "                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                    output_sequence_length=None, # how long should the output sequence of tokens be?\n",
        "                                    pad_to_max_tokens=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb3U0Zvk0EYG",
        "outputId": "4a662d01-0d3c-4c06-88ac-8ff9bb48d0b0"
      },
      "source": [
        "# Find average number of tokens (words) in training Tweets\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9rXuSBs0Pv7"
      },
      "source": [
        "# Setup text vectorization variables\n",
        "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
        "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length=max_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMSHqjnP0bcK"
      },
      "source": [
        "# Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vb6Pn8o0umS",
        "outputId": "1e0e920b-8ebe-4621-b0df-79e0b1a23edc"
      },
      "source": [
        "# Choose a random sentence from the training dataset and tokenize it\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "Officer Wounded Suspect Killed in Exchange of Gunfire: Richmond police officer wounded suspect killed in exc... http://t.co/zDHwRN6cZc      \n",
            "\n",
            "Vectorized version:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[ 677,  269,  430,  111,    4, 1861,    6, 2098, 1607,   77,  677,\n",
              "         269,  430,  111,    4]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CndTNIFo1Kwg",
        "outputId": "4395d44b-50b5-4d24-cb5b-59bdc9730988"
      },
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words: {top_5_words}\") \n",
        "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in vocab: 10000\n",
            "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
            "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsDx3ABy78QB"
      },
      "source": [
        "Creating an Embedding using an Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkq9esaI8AL3",
        "outputId": "7f43d8cb-c9c0-4581-cdb2-f6be5b731147"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length) # how long is each input\n",
        "\n",
        "embedding"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fc7202fb650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhYhPtjtHZKm",
        "outputId": "0d5f4253-cbd7-40fe-87d1-7a284f97d436"
      },
      "source": [
        "# Get a random sentence from training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "Suspect in latest theater attack had psychological issues http://t.co/3huhZxliiG      \n",
            "\n",
            "Embedded version:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.02812335, -0.0176071 ,  0.03786448, ...,  0.01181672,\n",
              "         -0.01387002, -0.00567365],\n",
              "        [-0.00872104,  0.0065316 ,  0.03304878, ..., -0.02280067,\n",
              "          0.0405825 ,  0.01073887],\n",
              "        [ 0.01852873,  0.00743871,  0.03585949, ...,  0.04838305,\n",
              "          0.03472928, -0.03265958],\n",
              "        ...,\n",
              "        [-0.01419514, -0.02437115,  0.04102958, ...,  0.02906257,\n",
              "          0.01228492, -0.03158795],\n",
              "        [-0.01419514, -0.02437115,  0.04102958, ...,  0.02906257,\n",
              "          0.01228492, -0.03158795],\n",
              "        [-0.01419514, -0.02437115,  0.04102958, ...,  0.02906257,\n",
              "          0.01228492, -0.03158795]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSZ-c9yeHg4S",
        "outputId": "613f5598-6041-4b44-f3d1-a65b0ce7a555"
      },
      "source": [
        "# Check out a single token's embedding\n",
        "sample_embed[0][0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              "array([ 0.02812335, -0.0176071 ,  0.03786448,  0.00695734, -0.00672927,\n",
              "        0.02763915,  0.0128667 , -0.01421726, -0.04793794, -0.01006224,\n",
              "        0.03066934,  0.00179293, -0.02682073,  0.04616796, -0.01939805,\n",
              "       -0.0458983 , -0.01844096, -0.03710938, -0.02878438,  0.03444291,\n",
              "       -0.02859312, -0.01718848,  0.01003831,  0.0329821 ,  0.01743137,\n",
              "       -0.00372002, -0.03440138, -0.03809752, -0.03343268, -0.01246266,\n",
              "       -0.03633636,  0.00937042,  0.0407165 ,  0.00897517,  0.03005395,\n",
              "       -0.01248194, -0.01402598, -0.0052564 , -0.00946089,  0.04279665,\n",
              "        0.03493786,  0.01278594, -0.04754704,  0.00219763, -0.0445423 ,\n",
              "       -0.02402159,  0.02927131, -0.03641327, -0.01755774,  0.00415776,\n",
              "        0.00480007, -0.04481507, -0.01972262,  0.03826382, -0.02756721,\n",
              "        0.04854279,  0.00749606,  0.01365519,  0.04790497, -0.00587201,\n",
              "        0.04838491, -0.04700221, -0.02711377, -0.02340456,  0.04698041,\n",
              "        0.03067008, -0.02681416,  0.02492232, -0.02422389,  0.01829102,\n",
              "       -0.02868449,  0.04410801, -0.04405363, -0.01902517,  0.00265864,\n",
              "       -0.03088489,  0.00139942,  0.03689481, -0.0317714 , -0.03029346,\n",
              "        0.03141708,  0.0250306 , -0.02695497, -0.01081668, -0.00694854,\n",
              "        0.01907314,  0.00520598,  0.01423795,  0.0049246 , -0.00281378,\n",
              "        0.0405396 , -0.02202821, -0.01446366, -0.00779628, -0.01511345,\n",
              "        0.04186821, -0.04955684, -0.01922376,  0.03206711, -0.01127049,\n",
              "        0.03738948,  0.02375165,  0.01948811,  0.00838686,  0.01816673,\n",
              "       -0.00860343, -0.03165803,  0.01420118,  0.00873014,  0.0491212 ,\n",
              "        0.038984  ,  0.04930304, -0.04766491,  0.03906142, -0.02851807,\n",
              "        0.01786507, -0.04819037, -0.00931985, -0.00400601, -0.04664179,\n",
              "        0.03545299,  0.01427973, -0.00113791, -0.0408618 ,  0.02646198,\n",
              "        0.01181672, -0.01387002, -0.00567365], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUGWpgUnHpQW"
      },
      "source": [
        "**Mode Building**\n",
        "We will begin with a baseline model and then compare it with other models.\n",
        "More specifically, we'll be building the following:\n",
        "\n",
        "    Model 0: Naive Bayes (baseline)\n",
        "    Model 1: Feed-forward neural network (dense model)\n",
        "    Model 2: LSTM model\n",
        "    Model 3: GRU model\n",
        "    Model 4: Bidirectional-LSTM model\n",
        "    Model 5: 1D Convolutional Neural Network\n",
        "    Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
        "    Model 7: Same as model 6 with 10% of training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUQDRmPYIEPO"
      },
      "source": [
        "Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n",
        "\n",
        "Each experiment will go through the following steps:\n",
        "\n",
        "    Construct the model\n",
        "    Train the model\n",
        "    Make predictions with the model\n",
        "    Track prediction evaluation metrics for later comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYqDhiPOJpyk"
      },
      "source": [
        "**Model 0**\n",
        "To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the Multinomial Naive Bayes algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQHcN_ENN2Tz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}